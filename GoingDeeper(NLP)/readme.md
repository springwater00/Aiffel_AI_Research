# Going Deeper Projects (NLP)

## GD2: Naver Movie Review Sentiment Analysis (mecab, okt, sentencepiece Performance Comparison)
**Purpose**: Sentiment analysis of Naver movie reviews using three different tokenization methods: mecab, okt, and sentencepiece
**Dataset**: Naver movie reviews and created a dataset consisting of labeled text data for sentiment analysis
**Results**: The performance of sentiment analysis using various tokenization methods has been compared, providing valuable insights into their effectiveness.

## GD4: Reuters News Classification (Performance Comparison of 8 ML Classifiers and DL Models)
**Purpose**: Text classification of Reuters news articles using eight different machine learning classifiers and deep learning models
**Dataset**: Reuters news dataset for text classification tasks
**Results**: A comprehensive evaluation of the classification performance of various models, both traditional machine learning and deep learning, is provided for informative comparisons.

## GD6: Bias Assessment in Word Embeddings Using WEAT Scores (Art vs. General Movie Genres)
**Purpose**: Assessed bias in word embeddings by calculating Word Embedding Association Test (WEAT) scores, focusing on distinguishing between artistic and general movie genres
**Dataset**: WEAT score analysis on word embeddings related to 21 different movie genres
**Results**: The project offers insights into the potential biases present in word embeddings related to movie genres, particularly those associated with art and general genres.

## GD8: Korean-English Translator (seq2seq)
**Purpose**: Building a Korean-English translation model using the seq2seq architecture
**Results**: The seq2seq-based model provides improved translation capabilities between Korean and English.

## GD10: Korean-English Translator (Transformer)
**Purpose**: Similar to GD8,  Korean-English translation but using Transformer model for enhanced performance
**Results**: The Transformer-based model provides improved translation capabilities between Korean and English.

## GD12: Chatbot (Transformer)
**Purpose**: The creation of a chatbot using the Transformer model, enabling interactive and intelligent conversational agents
**Result**s: The chatbot is developed using the Transformer architecture, enhancing its ability to engage in natural and meaningful conversations.

## GD14: MiniBERT
**Purpose**: MiniBERT is a compact version of the popular BERT model, designed for efficient natural language understanding tasks.
**Results**: This project introduces MiniBERT, providing a resource-efficient option for various NLP tasks while maintaining strong performance.

