{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ea6536",
   "metadata": {},
   "source": [
    "NLTK is a comprehensive toolkit for natural language processing tasks in multiple languages, and it includes tools and resources for many different tasks beyond just morphological analysis. While morphological analysis is an important part of NLP, it is just one of many subfields of NLP that NLTK covers.\n",
    "\n",
    "In terms of morphological analysis specifically, NLTK does provide tools for this task, such as a morphological stemmer that can be used to reduce words to their base form. NLTK also includes a part-of-speech (POS) tagger that can be used to identify the grammatical category of a word in a sentence, which is related to morphological analysis. However, NLTK is not limited to just morphological analysis and provides a wide range of other NLP tools and resources as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e9fa1",
   "metadata": {},
   "source": [
    "1. Tokenization - Breaking text into words, phrases, or sentences.  \n",
    "\n",
    "- 띄어쓰기 기준으로 parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4636c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.', 'It', 'has', 'words', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.word_tokenize(\"This is a sentence. It has words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cde65d",
   "metadata": {},
   "source": [
    "2. POS Tagging - Labeling the part of speech for each word in a sentence.  \n",
    "\n",
    "- tokenization + 품사구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f2f825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'), ('was', 'VBD'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(nltk.word_tokenize(\"This was a sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815d5e5",
   "metadata": {},
   "source": [
    "3. Stemming - Reducing words to their root form.\n",
    "\n",
    "- 문법 폼 떼고 어원만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f406ccbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjust'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")\n",
    "stemmer.stem(\"quickly\")\n",
    "stemmer.stem(\"adjustable\")\n",
    "# Output: 'run','quickli','adjust'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311642d7",
   "metadata": {},
   "source": [
    "4. Lemmatization - Similar to stemming but produces a more accurate base form of a word using linguistic rules.\n",
    "\n",
    "- 원형화.input을 모두 noun으로 간주하기 때문에 명사가 아니라면 form을 알려주고 파라미터로 함께 넣어줘야 한다. ran -> run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45388975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"ran\"\n",
    "pos = 'v'\n",
    "lemma = lemmatizer.lemmatize(word, pos)\n",
    "print(lemma)\n",
    "# Output: 'running','quickly'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c12ae7",
   "metadata": {},
   "source": [
    "5. Named Entity Recognition (NER) - Identifying and classifying named entities in text, such as people, organizations, and locations.\n",
    "\n",
    "- 문장을 tokenizing >> token별로 품사identifyiing >> 품사별로 classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadfc62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  44th/JJ\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "text = \"Barack Obama was the 44th President of the United States.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "# Output: (S (PERSON Barack/NNP Obama/NNP) was/VBD the/DT 44th/JJ President/NNP of/IN the/DT United/NNP States/NNPS ./.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04da3f5",
   "metadata": {},
   "source": [
    "# CountVectorizer (scikit-learn)\n",
    "\n",
    "- input : a collection of text documents (list of sentences, paragraphs, or even entire books)\n",
    "- function : tokenize, n-grams or counts the frequency  \n",
    "- output : row(documents) x columns(words,tokens) shaped sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9cc411",
   "metadata": {},
   "source": [
    "1. fit_transform(): a corpus of text documents and returns a document-term matrix, which represents the count of each word in each document. For "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cf6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"This is the first document.\", \"This is the second document.\", \"And this is the third one.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(doc_term_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f587933",
   "metadata": {},
   "source": [
    "2. get_feature_names(): This subfunction returns a list of all the unique words that were used to generate the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef613b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd54f1",
   "metadata": {},
   "source": [
    "3. vocabulary_: This subfunction returns a dictionary that maps each word to its index in the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e8aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985914b",
   "metadata": {},
   "source": [
    "# TruncatedSVD (scikit-learn)\n",
    "is a dimensionality reduction technique that can be used for matrix factorization, topic modeling, and feature extraction. It works by projecting high-dimensional data into a lower-dimensional space while preserving as much of the original variance as possible.\n",
    "\n",
    "1. fit: Compute the truncated singular value decomposition of a matrix and apply it to the data.\n",
    "\n",
    "2. transform: Apply dimensionality reduction to the input data.\n",
    "\n",
    "3. fit_transform: Fit the model with the input data and apply dimensionality reduction to it.\n",
    "\n",
    "4. inverse_transform: Transform low-dimensional data back to its original high-dimensional space.\n",
    "\n",
    "In this example, we first create a document-term matrix using CountVectorizer. We then apply TruncatedSVD to the document-term matrix to reduce its dimensionality to 5. We fit and transform the matrix using the fit_transform method of TruncatedSVD, which performs both steps in a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e81210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00000000e+00 -7.07106781e-01 -7.07106781e-01]\n",
      " [ 2.00000000e+00 -7.07106781e-01  7.07106781e-01]\n",
      " [ 2.00000000e+00  1.41421356e+00 -7.28840001e-16]]\n",
      "====================\n",
      "[[ 3.16643451e-16  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.36788191e-16  3.56682708e-16  1.00000000e+00  1.36788191e-16\n",
      "   1.00000000e+00]\n",
      " [ 1.56303455e-16  1.00000000e+00 -1.23869238e-16  1.00000000e+00\n",
      "   2.21402234e-17  1.00000000e+00  1.00000000e+00  2.21402234e-17\n",
      "   1.00000000e+00]\n",
      " [ 1.00000000e+00 -4.49289833e-16  6.43240206e-18  1.00000000e+00\n",
      "   1.00000000e+00 -2.39468092e-17  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit and transform the document-term matrix with TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "dtm_svd = svd.fit_transform(doc_term_matrix)\n",
    "print(dtm_svd) # 3차원\n",
    "print('='*20)\n",
    "dtm_svd = svd.inverse_transform(dtm_svd)\n",
    "print(dtm_svd) # 9차원"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
