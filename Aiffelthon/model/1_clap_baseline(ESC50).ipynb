{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a495772c",
   "metadata": {},
   "source": [
    "# 1. Extract only Encoder in detail (for fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f51fa",
   "metadata": {},
   "source": [
    ">## 1) Setting & Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4448875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: laion-clap in ./default/lib/python3.9/site-packages (1.1.4)\n",
      "Requirement already satisfied: transformers in ./default/lib/python3.9/site-packages (from laion-clap) (4.28.1)\n",
      "Requirement already satisfied: scikit-learn in ./default/lib/python3.9/site-packages (from laion-clap) (1.0.2)\n",
      "Requirement already satisfied: pandas in ./default/lib/python3.9/site-packages (from laion-clap) (1.4.4)\n",
      "Requirement already satisfied: progressbar in ./default/lib/python3.9/site-packages (from laion-clap) (2.5)\n",
      "Requirement already satisfied: torchlibrosa in ./default/lib/python3.9/site-packages (from laion-clap) (0.1.0)\n",
      "Requirement already satisfied: librosa in ./default/lib/python3.9/site-packages (from laion-clap) (0.10.0.post2)\n",
      "Requirement already satisfied: llvmlite in ./default/lib/python3.9/site-packages (from laion-clap) (0.39.1)\n",
      "Requirement already satisfied: scipy in ./default/lib/python3.9/site-packages (from laion-clap) (1.9.1)\n",
      "Requirement already satisfied: numpy==1.23.5 in ./default/lib/python3.9/site-packages (from laion-clap) (1.23.5)\n",
      "Requirement already satisfied: regex in ./default/lib/python3.9/site-packages (from laion-clap) (2022.7.9)\n",
      "Requirement already satisfied: ftfy in ./default/lib/python3.9/site-packages (from laion-clap) (6.1.1)\n",
      "Requirement already satisfied: tqdm in ./default/lib/python3.9/site-packages (from laion-clap) (4.64.1)\n",
      "Requirement already satisfied: soundfile in ./default/lib/python3.9/site-packages (from laion-clap) (0.12.1)\n",
      "Requirement already satisfied: webdataset in ./default/lib/python3.9/site-packages (from laion-clap) (0.2.48)\n",
      "Requirement already satisfied: wget in ./default/lib/python3.9/site-packages (from laion-clap) (3.2)\n",
      "Requirement already satisfied: h5py in ./default/lib/python3.9/site-packages (from laion-clap) (3.7.0)\n",
      "Requirement already satisfied: braceexpand in ./default/lib/python3.9/site-packages (from laion-clap) (0.1.7)\n",
      "Requirement already satisfied: wandb in ./default/lib/python3.9/site-packages (from laion-clap) (0.15.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in ./default/lib/python3.9/site-packages (from ftfy->laion-clap) (0.2.5)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (3.0.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (4.3.0)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (1.0.3)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (0.56.4)\n",
      "Requirement already satisfied: joblib>=0.14 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (1.1.0)\n",
      "Requirement already satisfied: pooch<1.7,>=1.0 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (1.6.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (0.3.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./default/lib/python3.9/site-packages (from librosa->laion-clap) (5.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./default/lib/python3.9/site-packages (from scikit-learn->laion-clap) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./default/lib/python3.9/site-packages (from soundfile->laion-clap) (1.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./default/lib/python3.9/site-packages (from pandas->laion-clap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./default/lib/python3.9/site-packages (from pandas->laion-clap) (2022.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (6.0)\n",
      "Requirement already satisfied: requests in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (0.13.3)\n",
      "Requirement already satisfied: filelock in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./default/lib/python3.9/site-packages (from transformers->laion-clap) (21.3)\n",
      "Requirement already satisfied: setuptools in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (63.4.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (4.22.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (5.9.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (3.1.31)\n",
      "Requirement already satisfied: setproctitle in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (1.3.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (0.4.0)\n",
      "Requirement already satisfied: pathtools in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (0.1.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (1.4.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (1.21.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in ./default/lib/python3.9/site-packages (from wandb->laion-clap) (8.0.4)\n",
      "Requirement already satisfied: pycparser in ./default/lib/python3.9/site-packages (from cffi>=1.0->soundfile->laion-clap) (2.21)\n",
      "Requirement already satisfied: six>=1.4.0 in ./default/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->laion-clap) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./default/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->laion-clap) (4.0.10)\n",
      "Requirement already satisfied: fsspec in ./default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->laion-clap) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./default/lib/python3.9/site-packages (from packaging>=20.0->transformers->laion-clap) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./default/lib/python3.9/site-packages (from requests->transformers->laion-clap) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./default/lib/python3.9/site-packages (from requests->transformers->laion-clap) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./default/lib/python3.9/site-packages (from requests->transformers->laion-clap) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./default/lib/python3.9/site-packages (from requests->transformers->laion-clap) (2022.9.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./default/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->laion-clap) (5.0.0)\n",
      "Requirement already satisfied: torch in ./default/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in ./default/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in ./default/lib/python3.9/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./default/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./default/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./default/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in ./default/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./default/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in ./default/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./default/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: jinja2 in ./default/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./default/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./default/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./default/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./default/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./default/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: wheel in ./default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in ./default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (63.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in ./default/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in ./default/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./default/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./default/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: torchvision in ./default/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: requests in ./default/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==2.0.0 in ./default/lib/python3.9/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: numpy in ./default/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./default/lib/python3.9/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: triton==2.0.0 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: sympy in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (1.10.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: filelock in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.6.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: networkx in ./default/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.8.4)\n",
      "Requirement already satisfied: wheel in ./default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (0.37.1)\n",
      "Requirement already satisfied: setuptools in ./default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (63.4.1)\n",
      "Requirement already satisfied: cmake in ./default/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.26.3)\n",
      "Requirement already satisfied: lit in ./default/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./default/lib/python3.9/site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./default/lib/python3.9/site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./default/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./default/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./default/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./default/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install laion-clap\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ca1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import laion_clap\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d4e71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLAP_Module',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'dir_path',\n",
       " 'hook',\n",
       " 'os',\n",
       " 'sys']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(laion_clap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd79cb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_audio_embedding_from_data',\n",
       " 'get_audio_embedding_from_filelist',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'get_text_embedding',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_ckpt',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'tokenizer',\n",
       " 'train',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(laion_clap.CLAP_Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d76b3",
   "metadata": {},
   "source": [
    ">## 2) Extract Audio Embedding\n",
    ": 총 2000개 데이터 중 100개만 넣었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34976b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1-13571-A-46.wav', '1-29532-A-16.wav', '5-260011-A-34.wav', '3-96606-B-49.wav', '5-160551-A-42.wav']\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 디렉토리 경로 지정\n",
    "dir_path = \"./data/ESC-50-master/audio\"\n",
    "\n",
    "# 디렉토리 내 파일 목록 불러오기\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "# 출력\n",
    "print(file_list[:5])\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5301536b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/ESC-50-master/audio/1-13571-A-46.wav', './data/ESC-50-master/audio/1-29532-A-16.wav', './data/ESC-50-master/audio/5-260011-A-34.wav', './data/ESC-50-master/audio/3-96606-B-49.wav', './data/ESC-50-master/audio/5-160551-A-42.wav', './data/ESC-50-master/audio/4-212698-A-39.wav', './data/ESC-50-master/audio/1-96890-A-37.wav', './data/ESC-50-master/audio/4-181286-A-10.wav', './data/ESC-50-master/audio/2-112213-A-39.wav', './data/ESC-50-master/audio/2-209475-A-25.wav']\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# prefix 추가\n",
    "prefix = './data/ESC-50-master/audio/'\n",
    "\n",
    "audio_file = [prefix + item for item in file_list]\n",
    "\n",
    "print(audio_file[:10]) \n",
    "print(len(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02869144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeong_soonju/default/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "# quantization\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    return (x * 32767.).astype(np.int16)\n",
    "\n",
    "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "model.load_ckpt() # download the default pretrained checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "921af636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03401422  0.06023627  0.07191774 ... -0.02979645 -0.04799221\n",
      "  -0.0649235 ]\n",
      " [-0.01128098 -0.011688    0.08284086 ... -0.03771019  0.0148686\n",
      "   0.01149836]\n",
      " [ 0.07900055  0.07890835  0.06254026 ... -0.07600399 -0.01828993\n",
      "   0.00212966]\n",
      " ...\n",
      " [ 0.02528274  0.00458613  0.03585381 ...  0.00787576  0.03576995\n",
      "   0.04022818]\n",
      " [ 0.01853131  0.03921445  0.02782208 ... -0.03592799  0.03818468\n",
      "   0.04554203]\n",
      " [ 0.02675372  0.01565943  0.0220779  ... -0.00032908  0.03575645\n",
      "   0.03166889]]\n",
      "(100, 512)\n"
     ]
    }
   ],
   "source": [
    "# Directly get audio embeddings from audio files\n",
    "\n",
    "audio_embed = model.get_audio_embedding_from_filelist(x = audio_file[:100], use_tensor=False)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21f060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03403534  0.06023425  0.07190047 -0.02228823 -0.03047081  0.01141005\n",
      "  -0.0611472   0.00043566  0.00295208  0.07440427  0.09748671  0.08374038\n",
      "   0.08714106  0.04601844 -0.04469379  0.03515762  0.03421513 -0.02977145\n",
      "  -0.04800081 -0.06491148]]\n",
      "(1, 512)\n"
     ]
    }
   ],
   "source": [
    "# Get audio embeddings from audio data\n",
    "audio_data, _ = librosa.load('./data/ESC-50-master/audio/1-13571-A-46.wav', sr=48000) # sample rate should be 48000\n",
    "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
    "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccda8",
   "metadata": {},
   "source": [
    "### 오디오 데이터가 너무 커서 임베딩할 때 커널이 죽음.. 그래서 100개만 슬라이스했음. 임베딩 할 때 어떻게 메모리 안터지고 불러올 수 있을까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc6b8e",
   "metadata": {},
   "source": [
    ">## 3) Extract Text Embedding\n",
    ": 총 2000개 중 100개만 넣었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186bf7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "['dog', 'chirping_birds', 'vacuum_cleaner', 'vacuum_cleaner', 'thunderstorm', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'door_wood_knock']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 DataFrame으로 로드\n",
    "df = pd.read_csv(\"./data/ESC-50-master/meta/esc50.csv\")\n",
    "\n",
    "# 'category' column의 모든 데이터를 list로 받음\n",
    "category_data = df['category'].tolist()\n",
    "\n",
    "# 출력\n",
    "print(len(category_data))\n",
    "print(category_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b876b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.51032587e-03 -1.21430203e-03  1.29887294e-02 ...  2.76181791e-02\n",
      "   7.74938148e-03  1.29231289e-02]\n",
      " [ 4.22641411e-02  8.13215151e-02 -1.10665656e-04 ... -2.58268900e-02\n",
      "  -2.80980468e-02 -5.83497286e-02]\n",
      " [ 3.94157786e-03  2.36528050e-02 -1.77783612e-02 ...  5.98671585e-02\n",
      "   3.54673155e-02  7.57417828e-03]\n",
      " ...\n",
      " [ 9.34299678e-02 -1.45519748e-02 -1.21943876e-01 ...  2.21117083e-02\n",
      "  -3.05675492e-02 -3.61665636e-02]\n",
      " [-5.67686297e-02  4.95181903e-02  1.54218050e-02 ...  1.63239948e-02\n",
      "   1.55661255e-02  7.64964102e-03]\n",
      " [ 1.31103229e-02 -1.66436210e-02  1.92866344e-02 ... -2.22927611e-02\n",
      "  -4.94182855e-03 -5.51377088e-02]]\n",
      "(100, 512)\n"
     ]
    }
   ],
   "source": [
    "# Get text embedings from texts:\n",
    "text_data = category_data\n",
    "text_embed = model.get_text_embedding(text_data[:100])\n",
    "print(text_embed)\n",
    "print(text_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8036411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01216932]\n",
      " [-0.03393425]\n",
      " [-0.1361347 ]\n",
      " [-0.1361347 ]\n",
      " [-0.0761677 ]\n",
      " [-0.0761677 ]\n",
      " [-0.02343627]\n",
      " [-0.17361444]\n",
      " [ 0.07817606]\n",
      " [-0.02343627]\n",
      " [-0.02343627]\n",
      " [ 0.09289318]\n",
      " [ 0.09289318]\n",
      " [ 0.09289318]\n",
      " [ 0.01216932]\n",
      " [ 0.09289318]\n",
      " [-0.0761677 ]\n",
      " [-0.05459817]\n",
      " [-0.05459819]\n",
      " [-0.05459819]\n",
      " [-0.05459819]\n",
      " [ 0.09289318]\n",
      " [ 0.09289318]\n",
      " [ 0.09289318]\n",
      " [ 0.0731846 ]\n",
      " [-0.00746779]\n",
      " [-0.11009359]\n",
      " [-0.08927881]\n",
      " [-0.03072681]\n",
      " [ 0.02010207]\n",
      " [-0.04015809]\n",
      " [-0.04015809]\n",
      " [-0.04015809]\n",
      " [ 0.45374185]\n",
      " [ 0.45374185]\n",
      " [ 0.12035933]\n",
      " [-0.09581985]\n",
      " [-0.07152376]\n",
      " [ 0.12035935]\n",
      " [ 0.03201603]\n",
      " [ 0.03201603]\n",
      " [ 0.03201603]\n",
      " [ 0.03201603]\n",
      " [ 0.03201603]\n",
      " [ 0.03201603]\n",
      " [-0.07007433]\n",
      " [-0.07007433]\n",
      " [-0.05459819]\n",
      " [-0.05459819]\n",
      " [ 0.13917448]\n",
      " [-0.04015809]\n",
      " [-0.11662252]\n",
      " [-0.11662252]\n",
      " [ 0.09437966]\n",
      " [-0.11471956]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.0531735 ]\n",
      " [ 0.03840921]\n",
      " [-0.11471956]\n",
      " [-0.1181232 ]\n",
      " [-0.11471956]\n",
      " [-0.11471956]\n",
      " [-0.11471956]\n",
      " [ 0.05893748]\n",
      " [-0.07007433]\n",
      " [ 0.09866319]\n",
      " [ 0.09866319]\n",
      " [-0.12006029]\n",
      " [-0.12006029]\n",
      " [-0.03651294]\n",
      " [-0.03651294]\n",
      " [ 0.01137589]\n",
      " [-0.11009358]\n",
      " [ 0.09814764]\n",
      " [-0.07007433]\n",
      " [-0.07007433]\n",
      " [-0.07007433]\n",
      " [-0.13541268]\n",
      " [ 0.09437966]\n",
      " [ 0.03491085]\n",
      " [ 0.03491085]\n",
      " [-0.1181232 ]\n",
      " [ 0.02010207]\n",
      " [ 0.02010207]\n",
      " [-0.1361347 ]\n",
      " [-0.1361347 ]\n",
      " [-0.1361347 ]\n",
      " [ 0.0731846 ]\n",
      " [ 0.0731846 ]\n",
      " [ 0.0731846 ]\n",
      " [ 0.04667141]\n",
      " [ 0.13917448]\n",
      " [ 0.07996956]\n",
      " [-0.18030605]\n",
      " [-0.10114377]]\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity\n",
    "norm_text = np.linalg.norm(text_embed, axis=1, keepdims=True)\n",
    "norm_audio = np.linalg.norm(audio_embed, axis=1, keepdims=True)\n",
    "dot_product = np.dot(text_embed, audio_embed.T)\n",
    "cos_similarity = dot_product / (norm_text * norm_audio.T)\n",
    "\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e2a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean cosine similarity: -0.016434615\n"
     ]
    }
   ],
   "source": [
    "# ESC50_Model_Cosine_Similarlity [:100]\n",
    "\n",
    "# Compute row-wise mean\n",
    "row_means = np.mean(cos_similarity, axis=1)\n",
    "# Compute overall mean\n",
    "overall_mean = np.mean(row_means)\n",
    "\n",
    "print(\"Overall mean cosine similarity:\", overall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d51aa",
   "metadata": {},
   "source": [
    "# 2. Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba7ef6",
   "metadata": {},
   "source": [
    ">## 1) Load Model (Dataset : ESC50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laion_clap\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "# download https://drive.google.com/drive/folders/1scyH43eQAcrBz-5fAw44C6RNBhC3ejvX?usp=sharing and extract ./ESC50_1/test/0.tar to ./ESC50_1/test/\n",
    "\n",
    "# Pick a Dataset type\n",
    "esc50_test_dir = 'data/0.tar/*/' # (flac-json)\n",
    "class_index_dict_path = 'data/ESC50_class_labels_indices_space.json'\n",
    "\n",
    "# Load the model\n",
    "model = laion_clap.CLAP_Module(enable_fusion=True) # device=device\n",
    "model.load_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class index dict\n",
    "class_index_dict = {v: k for v, k in json.load(open(class_index_dict_path)).items()}\n",
    "print('class 갯수:',len(class_index_dict))\n",
    "for key, value in list(class_index_dict.items())[:10]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57443c2",
   "metadata": {},
   "source": [
    ">## 2) Load Audio / Text / Ground-truth files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0411db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data\n",
    "import tarfile\n",
    "\n",
    "# 압축 파일 해제\n",
    "with tarfile.open('data/0.tar') as tar:\n",
    "    tar.extractall('data/ESC50_1/test/')\n",
    "\n",
    "# flac 파일 경로 가져오기\n",
    "audio_files = sorted(glob.glob('data/ESC50_1/test/**/*.flac', recursive=True))\n",
    "\n",
    "print('audio files 갯수:',len(audio_files)) # 여러개 파일 \n",
    "print(audio_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = sorted(glob.glob('./data/ESC50_1/test/**/*.json', recursive=True))\n",
    "print('json_files 갯수:',len(json_files))\n",
    "print(json_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_idx = [class_index_dict[json.load(open(jf))['category']] for jf in json_files]\n",
    "print('ground_truth_idx 갯수:',len(ground_truth_idx))\n",
    "print(ground_truth_idx[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a496de2",
   "metadata": {},
   "source": [
    ">## 3) Zeroshot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08890e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "torchaudio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28027434",
   "metadata": {},
   "source": [
    ">>### (1) Audio/Text Embedding 값 불러오기 & 간단스크립트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c745c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ground_truth = torch.tensor(ground_truth_idx).view(-1, 1)\n",
    "\n",
    "    # Get text features\n",
    "    all_texts = [\"This is a sound of \" + t for t in class_index_dict.keys()]\n",
    "    text_embed = model.get_text_embedding(all_texts)\n",
    "    audio_embed = model.get_audio_embedding_from_filelist(x=audio_files)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53a874",
   "metadata": {},
   "source": [
    "- ground_truth_idx 리스트(classification 답)를 PyTorch 텐서로 변환하고,.view로 차원을 맞춰둠.\n",
    "- all texts 인자에 class_index_dict의 key값(text)를 받아 확인할 수 있게 하고,\n",
    "- text인 all texts 리스트에 대한 임베딩 값 text_embed 얻고,\n",
    "- audio files 리스트에 있는 모든 오디오 파일에 대한 audio_embed 얻음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881aff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('text_embed length :',len(text_embed)) # class 갯수\n",
    "    print('text_embed shape :',text_embed.shape)\n",
    "    print(text_embed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('audio_embed length :',len(audio_embed)) # 오디오파일 갯수\n",
    "    print('audio_embed shape :',audio_embed.shape)\n",
    "    print(audio_embed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9e033",
   "metadata": {},
   "source": [
    ">>### (2) Contrastive Learning & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f02060",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ranking = torch.argsort(torch.tensor(audio_embed) \n",
    "                            @ torch.tensor(text_embed).t(), \n",
    "                            descending=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b51d3",
   "metadata": {},
   "source": [
    "1. audio_embed & text_embed.T 내적곱해서 latent space 통일\n",
    "2. torch.argsort : 계산한 내적값 작은순으로 sort해서 다시 index 부여\n",
    "3. 다시 내림차순으로 정렬  \n",
    "**>>> ranking = 음성과 텍스트 데이터간 유사도가 높은 순서대로 정렬된 인덱스가 담김**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    preds = torch.where(ranking == ground_truth)[1]\n",
    "    preds = preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273185c",
   "metadata": {},
   "source": [
    "1. 가장 유사하다고 판단한 ranking 값과 실제답 ground_truth가 같으면 true를 반환함\n",
    "2. true값의 위치를 찾아 주는 것이 torch.where\n",
    "3.  [1] = 인덱스값   \n",
    "**>>> preds = 음성과 텍스트간의 유사도가 가장 높은 class name(text)를 담음**\n",
    "4. tensor를 np로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137d926",
   "metadata": {},
   "source": [
    ">>### (3) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    metrics = {}\n",
    "    metrics[f\"mean_rank\"] = preds.mean() + 1\n",
    "    metrics[f\"median_rank\"] = np.floor(np.median(preds)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53677c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(metrics[f\"mean_rank\"])\n",
    "    print(metrics[f\"median_rank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988a909",
   "metadata": {},
   "source": [
    "1. 결과값을 담을 metrics dictionary 열어두고\n",
    "2. preds의 평균값과 중앙값을 구해서 순위를 0이 아닌 1부터 시작하게함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da621f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for k in [1, 5, 10]:\n",
    "        metrics[f\"R@{k}\"] = np.mean(preds < k)\n",
    "    # map@10\n",
    "    metrics[f\"mAP@10\"] = np.mean(np.where(preds < 10, 1 / (preds + 1), 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7a25e",
   "metadata": {},
   "source": [
    "- R@1,5,10 = 예측된 순위(preds)가 실상위 k개 안에 있으면 1, 아니면 0 한 뒤 평균\n",
    "- mAP@10 = Mean Average Precision at 10 : 모델이 상위 10개 예측에 대해 얼마나 정확한 결과를 내는지 평가하는 지표\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48042bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\n",
    "        f\"Zeroshot Classification Results: \"\n",
    "        + \"\\t\".join([f\"{k}: {round(v, 4):.4f}\" for k, v in metrics.items()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5470c",
   "metadata": {},
   "source": [
    "- mean_rank: 1.2425 : 모델이 예측한 결과 중 상위 2개 클래스가 정답 클래스로 예측되었음.\n",
    "- median_rank: 1.0 : 중앙값으로 예측해도 평균적으로 1이라는 것은 대체로 예측된 클래스 중 1위 클래스가 실제값이라는 뜻.\n",
    "- R@1,5,10 : 예측한 1,5,10위까지의 순위 안에 정답이 포함될 확률\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4b9ff",
   "metadata": {},
   "source": [
    ">>### (4) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ranking))\n",
    "print(type(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d39cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    # num_classes x num_classes 크기의 0으로 채워진 행렬 생성\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # 행렬의 각 [i][j] 요소는 i번 클래스가 j번 클래스로 분류된 샘플의 수\n",
    "    for i in range(len(y_true)):\n",
    "        true_idx = y_true[i]\n",
    "        pred_idx = y_pred[i]\n",
    "        conf_mat[true_idx][pred_idx] += 1\n",
    "\n",
    "    return conf_mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, class_names):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation=90)  # x축 레이블을 90도 기울임\n",
    "    plt.show()\n",
    "\n",
    "# confusion matrix 생성\n",
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, torch.argmax(ranking, dim=1), num_classes)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "plot_confusion_matrix(conf_mat, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cae25",
   "metadata": {},
   "source": [
    "hand saw - dog(8) / cat - hand saw(6) / chainsaw - rooster(6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "wav_path = './data/dog.wav'\n",
    "audio = AudioSegment.from_file(wav_path, format=\"wav\")\n",
    "play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914e88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    # num_classes x num_classes 크기의 0으로 채워진 행렬 생성\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # 행렬의 각 [i][j] 요소는 i번 클래스가 j번 클래스로 분류된 샘플의 수\n",
    "    for i in range(len(y_true)):\n",
    "        true_idx = y_true[i]\n",
    "        pred_idx = y_pred[i]\n",
    "        conf_mat[true_idx][pred_idx] += 1\n",
    "\n",
    "    return conf_mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, class_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(conf_mat, interpolation='nearest', cmap='Blues')\n",
    "    \n",
    "    # x, y 축 설정\n",
    "    ax.set(xticks=np.arange(conf_mat.shape[1]), yticks=np.arange(conf_mat.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    # x 축 레이블 90도 회전\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # 각 셀에 값 출력\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            text = ax.text(j, i, conf_mat[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    # colorbar 추가\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # 그래프 보여주기\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# confusion matrix 생성\n",
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, torch.argmax(ranking, dim=1), num_classes)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "plot_confusion_matrix(conf_mat, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9148679",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf12828",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, ranking, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# 오차행렬 생성\n",
    "conf_mat = multilabel_confusion_matrix(ground_truth, ranking)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 예측값과 실제값이 일치하는지 계산\n",
    "correct_preds = (ranking == ground_truth)\n",
    "print(correct_preds.shape)\n",
    "incorrect_preds = (ranking != ground_truth)\n",
    "incorrect_ranking = ranking[incorrect_preds][0]\n",
    "print(incorrect_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 오차행렬 생성\n",
    "conf_mat = confusion_matrix(ground_truth[incorrect_preds], ranking[incorrect_preds])\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 오차행렬 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, ax=ax)\n",
    "\n",
    "# x축, y축 라벨 설정\n",
    "ax.set_xlabel('Predicted labels', fontsize=16)\n",
    "ax.set_ylabel('True labels', fontsize=16)\n",
    "ax.set_xticklabels(class_names, fontsize=12)\n",
    "ax.set_yticklabels(class_names, fontsize=12)\n",
    "\n",
    "# 그래프 제목 설정\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e95ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일치하는 경우 0, 그렇지 않은 경우 1 array 만들기\n",
    "color_indices = np.where(correct_preds, 0, 1)\n",
    "color_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 100))\n",
    "im = ax.imshow(color_indices, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xticks(np.arange(color_indices.shape[1]))\n",
    "ax.set_yticks(np.arange(color_indices.shape[0]))\n",
    "ax.set_xticklabels(class_index_dict, fontsize=8)\n",
    "ax.set_yticklabels(range(400), fontsize=8)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce132bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fcbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [class_index_dict[i] for i in preds]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f65683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class_index_dict_path = 'data/ESC50_class_labels_indices_space.json'\n",
    "class_index_dict = {v: k for v, k in json.load(open(class_index_dict_path)).items()}\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'preds': preds,\n",
    "    'ground_truth': ground_truth.squeeze().tolist(),\n",
    "    'audio_file': audio_files,\n",
    "    'actual_class': [class_index_dict[i] for i in ground_truth.squeeze().tolist()],\n",
    "    'predicted_class': [class_index_dict[i] for i in preds]\n",
    "})\n",
    "\n",
    "df_diff = df[df['actual_class'] != df['predicted_class']]\n",
    "display(df_diff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f3fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
