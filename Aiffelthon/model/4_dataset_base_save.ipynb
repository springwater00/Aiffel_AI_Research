{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f208f62",
   "metadata": {},
   "source": [
    "# 1. Extract only Encoder in detail (for fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cab6d",
   "metadata": {},
   "source": [
    ">## 1) Setting & Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bab524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: laion-clap in /opt/conda/lib/python3.9/site-packages (1.1.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from laion-clap) (4.62.3)\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.10.3.post1)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.9/site-packages (from laion-clap) (6.1.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from laion-clap) (1.10.1)\n",
      "Requirement already satisfied: numpy==1.23.5 in /opt/conda/lib/python3.9/site-packages (from laion-clap) (1.23.5)\n",
      "Requirement already satisfied: braceexpand in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.1.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from laion-clap) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from laion-clap) (1.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.15.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (from laion-clap) (3.1.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from laion-clap) (2021.11.10)\n",
      "Requirement already satisfied: progressbar in /opt/conda/lib/python3.9/site-packages (from laion-clap) (2.5)\n",
      "Requirement already satisfied: wget in /opt/conda/lib/python3.9/site-packages (from laion-clap) (3.2)\n",
      "Requirement already satisfied: torchlibrosa in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.1.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from laion-clap) (4.11.3)\n",
      "Requirement already satisfied: webdataset in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.2.48)\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.8.1)\n",
      "Requirement already satisfied: llvmlite in /opt/conda/lib/python3.9/site-packages (from laion-clap) (0.36.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.9/site-packages (from ftfy->laion-clap) (0.2.5)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (2.1.9)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (4.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (21.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (1.1.0)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (0.53.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (1.5.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.9/site-packages (from librosa->laion-clap) (0.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->laion-clap) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.9/site-packages (from soundfile->laion-clap) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->laion-clap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->laion-clap) (2021.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (0.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (0.0.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->laion-clap) (3.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (1.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (8.0.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (3.19.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (3.1.31)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (5.8.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (1.4.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (59.4.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (4.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->laion-clap) (1.22.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0->soundfile->laion-clap) (2.21)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->laion-clap) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->laion-clap) (4.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->librosa->laion-clap) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->laion-clap) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->laion-clap) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->laion-clap) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->laion-clap) (1.26.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->laion-clap) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (0.10.1+cu111)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision) (8.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: torch==1.9.1 in /opt/conda/lib/python3.9/site-packages (from torchvision) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.9.1->torchvision) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install laion-clap\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee53c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import laion_clap\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a61398",
   "metadata": {},
   "source": [
    ">## 2) Extract Audio Embedding\n",
    ": 총 2000개 데이터 중 100개만 넣었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b2ceb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 디렉토리 경로 지정\n",
    "dir_path = \"ESC-50-master/ESC-50-master/audio\"\n",
    "\n",
    "# 디렉토리 내 파일 목록 불러오기\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "# # 출력\n",
    "# print(file_list[:5])\n",
    "# print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47078521",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prefix 추가\n",
    "prefix = 'ESC-50-master/ESC-50-master/audio/'\n",
    "\n",
    "audio_file = [prefix + item for item in file_list]\n",
    "\n",
    "# print(audio_file[:10]) \n",
    "# print(len(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6fdf24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "Downloading laion_clap weight files...\n",
      "Download completed!\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "# quantization\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    return (x * 32767.).astype(np.int16)\n",
    "\n",
    "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "model.load_ckpt() # download the default pretrained checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b1f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.33158506e-02  5.45929149e-02  7.16352537e-02  1.31517705e-02\n",
      "   3.05129252e-02 -6.31129518e-02  1.49855511e-02  4.42864113e-02\n",
      "  -4.53110784e-02 -6.76504523e-02 -1.26454774e-02 -5.57375327e-02\n",
      "   3.73854488e-02  4.09765616e-02 -7.31308386e-02  2.02682018e-02\n",
      "   1.49454279e-02 -3.83634567e-02  3.47014628e-02 -1.23500060e-02]\n",
      " [ 1.39670111e-02  1.38229467e-02 -2.11973232e-03 -1.91621892e-02\n",
      "  -2.11460870e-02 -4.33889106e-02 -4.62117679e-02 -2.74212155e-02\n",
      "  -1.58375371e-02 -4.47046012e-02 -2.58371942e-02  6.59703985e-02\n",
      "   1.22593306e-01  2.69965883e-02  1.22840796e-03 -6.60994127e-02\n",
      "  -3.86373177e-02 -4.40261047e-03 -3.93143203e-03  2.89975088e-02]\n",
      " [ 8.36510807e-02  1.01906650e-01  7.18332306e-02  6.55123964e-03\n",
      "   6.47192495e-03 -2.05253921e-02  3.49606834e-02  4.27918509e-02\n",
      "  -1.62978377e-02 -2.33648866e-02 -6.58885688e-02  6.45386847e-03\n",
      "  -3.60516682e-02  3.38612171e-03 -3.21768709e-02 -1.69701669e-02\n",
      "   4.25500944e-02  1.34484079e-02 -1.62685253e-02 -3.72272357e-03]\n",
      " [ 1.23938471e-01  6.78174421e-02  5.67109734e-02  2.22206637e-02\n",
      "  -5.91825247e-02  4.03291993e-02 -2.26554964e-02 -6.41138013e-03\n",
      "  -4.25072834e-02 -4.76332977e-02  2.86890334e-03 -2.78067179e-02\n",
      "   5.80760688e-02  4.85768244e-02 -1.52724400e-01 -1.29908405e-03\n",
      "  -1.36563769e-02 -8.67686141e-03 -1.63333211e-02 -6.54173791e-02]\n",
      " [-1.54873691e-02  2.64928062e-02  6.62292838e-02  6.77889213e-02\n",
      "  -4.07118574e-02  2.17117239e-02  6.42449595e-03 -4.60132323e-02\n",
      "  -6.21105358e-02 -6.52214214e-02  2.78477883e-03 -1.90176852e-02\n",
      "   2.36397441e-02  6.23244159e-02 -4.52822968e-02  1.73534267e-03\n",
      "  -7.07950592e-02 -1.71776246e-02  2.54191365e-02  4.42842506e-02]\n",
      " [ 5.69782220e-02  2.13618260e-02  6.89900666e-02  6.21939786e-02\n",
      "  -2.52342746e-02  3.42215076e-02 -2.77733132e-02  4.03060727e-02\n",
      "  -5.93195744e-02  1.27232941e-02  2.16778973e-03 -1.11497752e-01\n",
      "  -1.26916626e-02  3.37563381e-02  5.75083867e-03  6.45402968e-02\n",
      "  -8.20534711e-04  1.28492769e-02  8.64343066e-03  4.02862430e-02]\n",
      " [ 6.16322877e-03  9.14161280e-02  7.03520374e-04 -2.88630947e-02\n",
      "  -8.45200121e-02  1.98121164e-02 -4.24721129e-02 -6.38757274e-02\n",
      "   9.31041464e-02 -5.32856919e-02  3.65441572e-03 -1.25191305e-02\n",
      "   6.49922192e-02  5.63416742e-02 -7.96778053e-02  3.94593738e-02\n",
      "   2.23883148e-02 -6.22852929e-02 -2.73709241e-02  1.25085339e-02]\n",
      " [ 3.77960093e-02 -1.82864685e-02  2.52445824e-02  3.49105932e-02\n",
      "  -3.02660149e-02 -3.22981924e-02  3.34697543e-03  4.34190631e-02\n",
      "  -6.53622299e-02 -2.21280493e-02 -9.67597663e-02 -2.22736783e-02\n",
      "   1.58093125e-02  6.61970526e-02  4.17991728e-02 -3.83156724e-02\n",
      "  -2.97612213e-02 -3.59735712e-02  2.47084675e-03  4.53006960e-02]\n",
      " [ 3.19439848e-03  7.63810053e-02  9.85613540e-02  3.54643306e-03\n",
      "  -3.07166353e-02 -1.34223728e-02 -1.52534079e-02 -3.48202400e-02\n",
      "  -2.88958978e-02 -5.25200814e-02 -3.67004275e-02  3.19192484e-02\n",
      "   5.73417619e-02  7.24201575e-02 -6.28703907e-02 -7.30749313e-03\n",
      "   4.02789935e-02 -6.53519183e-02 -1.34669319e-02 -2.62053888e-02]\n",
      " [ 2.69598216e-02  3.93686667e-02  7.47487247e-02  3.74150537e-02\n",
      "  -4.07207385e-03 -9.00668502e-02  9.81405843e-03  6.74163271e-03\n",
      "  -1.54328234e-02 -5.60671166e-02 -4.70648296e-02  1.30794525e-01\n",
      "   1.55114587e-02  6.88511580e-02 -4.54567187e-02 -3.06090340e-02\n",
      "  -1.29411323e-02 -5.50027192e-02 -6.81129098e-02  5.23834452e-02]\n",
      " [ 9.63901505e-02  3.11914142e-02  4.91903760e-02 -1.06333457e-02\n",
      "  -6.38689250e-02  2.05029976e-02 -1.72901470e-02  8.84966273e-03\n",
      "  -3.55400029e-03 -9.06147622e-03 -1.07346158e-02  3.77894379e-02\n",
      "   3.50667648e-02  1.24649238e-02 -6.51988536e-02 -9.14826989e-03\n",
      "  -4.63669784e-02 -2.99118040e-03 -3.87686528e-02  1.43211698e-02]\n",
      " [ 9.60464254e-02  6.77407607e-02  5.19602336e-02  2.22607516e-02\n",
      "  -8.09159800e-02  5.37357815e-02  1.69257373e-02 -2.49452684e-02\n",
      "  -3.71898673e-02 -4.19910550e-02  1.13625061e-02  9.08587128e-03\n",
      "   5.33950478e-02  7.64344633e-02 -1.29246742e-01 -7.65506236e-04\n",
      "   1.31692709e-02 -2.57604737e-02 -3.03449146e-02 -4.86738421e-02]\n",
      " [ 8.23522285e-02 -4.09909636e-02  9.37900916e-02  3.89775150e-02\n",
      "   2.48894356e-02 -2.47154869e-02  6.36722296e-02  6.57456517e-02\n",
      "  -8.72354861e-03 -4.24952246e-02 -1.46880448e-02  2.08587050e-02\n",
      "   8.62654671e-02 -7.04387203e-04 -5.45039885e-02  1.87993348e-02\n",
      "  -2.07707714e-02 -3.80165651e-02 -5.28719798e-02 -8.76527429e-02]\n",
      " [ 3.86165418e-02  5.17354384e-02  8.41706246e-02  2.68340521e-02\n",
      "  -2.46343873e-02 -4.33835238e-02  3.16105522e-02 -4.28114720e-02\n",
      "  -6.86290786e-02  3.30246203e-02 -9.13085565e-02  5.67430034e-02\n",
      "   3.17570120e-02  2.76346281e-02  5.03586270e-02 -5.02286591e-02\n",
      "  -6.57345820e-03 -6.84178099e-02 -6.28196970e-02 -6.00423664e-04]\n",
      " [ 4.28352132e-02 -6.42030984e-02  3.83106731e-02 -8.72153044e-02\n",
      "  -7.36317364e-03  3.56218666e-02 -1.09727914e-02 -1.23405559e-02\n",
      "   7.09470809e-02 -2.24499987e-03 -4.69478369e-02  1.05742635e-02\n",
      "   4.88565639e-02  4.85252254e-02  5.50002120e-02 -1.76423742e-03\n",
      "  -5.17681651e-02 -1.63303986e-02 -7.45246112e-02 -1.91367920e-02]\n",
      " [-5.42474166e-02  5.16435393e-05  5.62723614e-02 -1.71375182e-02\n",
      "  -7.21416026e-02  3.16083878e-02  2.27710903e-02 -1.38969850e-02\n",
      "  -7.16217933e-03 -3.10782604e-02 -5.24764657e-02 -9.97203514e-02\n",
      "   5.62804714e-02  3.38850804e-02  2.61043608e-02  7.32064471e-02\n",
      "  -1.66072361e-02  2.29925755e-02  6.33927062e-02  3.30258347e-02]\n",
      " [ 2.56150849e-02 -2.79645454e-02  9.62973665e-03 -2.47525480e-02\n",
      "  -6.47244835e-03 -4.35113683e-02 -1.44105940e-03 -4.21517231e-02\n",
      "  -3.94917578e-02  4.41859774e-02 -5.53205684e-02  3.32452990e-02\n",
      "   2.73736590e-03  1.30010992e-01 -7.21348450e-02  5.51729240e-02\n",
      "   2.55026463e-02 -4.95240092e-02 -2.54147481e-02 -3.99338938e-02]\n",
      " [ 2.69783195e-04  3.90432775e-02  7.42200464e-02  5.94302155e-02\n",
      "   3.77905741e-02 -4.26451787e-02  3.20219970e-03 -2.83136424e-02\n",
      "  -7.57880602e-03 -6.47800565e-02  2.40797587e-02  1.55989826e-02\n",
      "   6.45411015e-02 -3.72206699e-03 -1.00556664e-01 -2.39260737e-02\n",
      "  -5.07386588e-02 -6.98383600e-02  4.66497689e-02 -2.62620207e-02]\n",
      " [ 1.13435455e-01  3.62387225e-02  6.13786243e-02  3.02625615e-02\n",
      "  -4.83037792e-02  1.76212620e-02 -2.68757832e-03 -1.02224608e-03\n",
      "  -3.81727517e-02 -6.45197928e-02  2.64004953e-02 -4.56920080e-02\n",
      "   6.83264285e-02  5.67541681e-02 -1.56532481e-01  1.33634743e-03\n",
      "  -1.95791014e-03 -1.89327374e-02 -4.04515006e-02 -2.19805650e-02]\n",
      " [ 1.86839607e-02  8.05006027e-02  9.56833884e-02 -4.74526100e-02\n",
      "   4.51856218e-02 -5.58903925e-02 -2.58073024e-03 -1.67997740e-02\n",
      "  -2.83383969e-02  4.17456254e-02 -4.46109986e-03  2.55133770e-02\n",
      "   7.76310191e-02 -3.02154850e-02 -6.00171909e-02 -2.82700900e-02\n",
      "  -8.05540681e-02  9.77487769e-03 -7.58075416e-02  9.59127862e-03]\n",
      " [ 4.16662395e-02  4.92402464e-02  6.37759119e-02  2.27312185e-02\n",
      "  -1.62434988e-02 -2.11905316e-02 -2.04889178e-02 -2.28137802e-02\n",
      "  -6.51261508e-02 -3.41416933e-02 -3.73682044e-02  9.27752331e-02\n",
      "   1.53345950e-02  6.75286651e-02 -8.50405544e-03 -1.30081000e-02\n",
      "   9.89660434e-03 -3.32793854e-02  2.02361625e-02  1.45081822e-02]\n",
      " [ 2.70866863e-02 -1.33895129e-02  9.80124772e-02 -1.03498816e-01\n",
      "  -4.42304537e-02  1.18010705e-02  2.46252073e-03  2.82675237e-03\n",
      "   2.33784690e-03 -4.01115119e-02 -1.91522650e-02  3.23344320e-02\n",
      "   1.07885636e-01  3.74686904e-02 -1.18391123e-02 -9.92765464e-03\n",
      "   6.20775810e-03 -5.24366535e-02  3.08583025e-02 -5.45932017e-02]\n",
      " [ 3.00300326e-02  5.88705167e-02  8.34255442e-02  2.66638957e-02\n",
      "  -1.10767901e-01  1.19852945e-02 -4.06596847e-02 -1.70215089e-02\n",
      "   2.25012563e-02 -2.46533919e-02 -6.63384330e-04  5.82861528e-02\n",
      "   4.57074493e-02  1.95638351e-02 -1.18759423e-02  5.30618709e-03\n",
      "   1.98351666e-02 -2.05037072e-02 -2.36969013e-02 -2.47847065e-02]\n",
      " [-1.99179375e-03  3.14911902e-02  5.93277104e-02 -3.65285063e-03\n",
      "  -4.47855815e-02  9.43544600e-03 -1.76054845e-03 -3.33869793e-02\n",
      "  -3.20123062e-02 -2.01372225e-02 -8.49017967e-03  4.82935794e-02\n",
      "   2.43838299e-02  1.04905769e-01 -4.77756634e-02  1.84267238e-02\n",
      "   4.86791730e-02 -4.31577899e-02 -2.48248708e-02 -7.83500746e-02]\n",
      " [ 2.92779319e-02  6.86753914e-02  7.19600171e-02  4.27283570e-02\n",
      "  -6.33143857e-02 -6.67584501e-03  5.53222094e-03 -1.51319606e-02\n",
      "   1.65506676e-02 -5.88810593e-02 -1.09665468e-02  2.06269566e-02\n",
      "   7.05036670e-02  4.09461558e-02 -7.85322636e-02 -2.47083120e-02\n",
      "   4.36443165e-02 -6.56539947e-02 -3.72597240e-02 -4.21219766e-02]\n",
      " [-5.92014045e-02  3.17716859e-02  2.41367444e-02  1.06233368e-02\n",
      "   4.59998250e-02 -8.69367421e-02  6.08221022e-03 -1.92803331e-02\n",
      "   4.87926835e-03 -9.09807160e-02 -6.88541820e-03 -3.87603119e-02\n",
      "   8.47258430e-04  6.15938194e-02 -3.12780887e-02  3.71891772e-03\n",
      "  -2.00320277e-02  2.94451136e-02  5.67411520e-02 -3.27045657e-02]\n",
      " [ 3.47466059e-02  7.86506981e-02  2.13161274e-03 -2.13729143e-02\n",
      "   7.49477418e-03 -1.09286066e-02 -3.41115892e-02 -7.33229592e-02\n",
      "  -4.51554125e-03 -5.33422967e-03  5.22739142e-02 -3.80934142e-02\n",
      "   1.03345588e-01  6.46723136e-02 -6.21777773e-02 -4.74161692e-02\n",
      "   7.35412398e-03 -5.68338819e-02 -5.63905723e-02  1.76863186e-02]\n",
      " [ 4.40680757e-02  7.47735426e-02  8.18240643e-02  6.43526614e-02\n",
      "  -1.82482600e-02  1.69390850e-02  3.30719873e-02 -6.15669927e-03\n",
      "  -4.43259766e-03 -1.54493134e-02  1.26337530e-02 -2.77642775e-02\n",
      "  -5.11603206e-02  1.32940747e-02 -6.38896674e-02  1.02551281e-01\n",
      "   9.12764692e-04 -7.01270252e-02 -3.26120318e-03  6.15863875e-03]\n",
      " [ 2.53477879e-02  4.50294688e-02  2.35163495e-02 -5.04163578e-02\n",
      "  -2.47875676e-02  6.17258586e-02 -3.34916189e-02  5.23770265e-02\n",
      "   2.38206778e-02  2.46884637e-02 -2.85398457e-02  4.24195454e-02\n",
      "   5.74252903e-02  2.64909095e-03  8.83908477e-03 -5.39131165e-02\n",
      "   1.88692566e-02 -3.72129865e-02 -2.00395212e-02 -2.15069819e-02]\n",
      " [-5.11046723e-02  2.38952544e-02  1.01175278e-01  2.83517465e-02\n",
      "   2.62049455e-02 -5.88697828e-02  2.79992446e-02  1.69439940e-04\n",
      "  -7.60599924e-03 -1.20983915e-02 -7.84812123e-02 -2.22791896e-05\n",
      "   2.25027911e-02  8.65452515e-04 -7.34652206e-02  1.14483267e-01\n",
      "   5.34717217e-02  1.17310323e-02 -5.87363029e-03  5.00325188e-02]\n",
      " [ 6.61982968e-02  5.91300763e-02  4.73563559e-02  1.07561834e-02\n",
      "  -6.12860434e-02  3.77467424e-02  1.32517994e-03  1.51311290e-02\n",
      "  -8.32873285e-02 -2.26775613e-02 -2.06380077e-02  4.58561480e-02\n",
      "   1.47357639e-02 -1.25193028e-02  6.08534180e-03 -7.06228602e-04\n",
      "   7.51468958e-03  1.15450574e-02 -4.70367074e-03  6.28326386e-02]\n",
      " [ 8.68362188e-02  9.02448148e-02  9.63619873e-02  4.80729342e-03\n",
      "   2.06147153e-02 -3.17278020e-02 -5.44261560e-02 -5.64424358e-02\n",
      "  -6.84289485e-02  2.92442087e-03 -1.99361728e-03  1.94956511e-02\n",
      "   3.68536785e-02  2.77706403e-02  2.05663685e-03 -1.68958139e-02\n",
      "  -2.22562198e-02 -4.97800484e-02 -2.74801273e-02  1.87471509e-02]\n",
      " [-9.98406950e-03  4.45865393e-02 -2.50414237e-02 -1.46245705e-02\n",
      "  -2.62332186e-02  6.38018250e-02  1.24213342e-02  1.88667569e-02\n",
      "   3.70129012e-02  4.01133634e-02 -1.78072657e-02 -4.98312712e-02\n",
      "   2.67176796e-02  4.35261652e-02 -9.66447312e-03  7.28041260e-03\n",
      "  -9.67308693e-03 -8.33170861e-02 -2.59963125e-02  2.49147993e-02]\n",
      " [ 3.58072221e-02  1.27416264e-04  6.45520166e-02  4.91278023e-02\n",
      "  -1.10553717e-02  2.08178107e-02 -5.67819166e-04 -3.67495790e-02\n",
      "  -6.52453303e-02 -6.11851290e-02 -4.63891812e-02 -1.75560210e-02\n",
      "   3.68649736e-02  4.12358865e-02 -4.86228317e-02 -1.77875329e-02\n",
      "  -7.07258508e-02 -9.30703711e-04  2.01039109e-02  3.32202017e-02]\n",
      " [-9.11541574e-04  3.70786153e-03  3.37753259e-02  1.75732421e-03\n",
      "   4.26020920e-02 -2.30048150e-02 -3.00700609e-02 -7.27998316e-02\n",
      "  -6.02451293e-03 -7.68257752e-02  8.06389470e-03  3.08993589e-02\n",
      "   3.23072374e-02  6.15567304e-02 -8.31127837e-02 -4.72301431e-03\n",
      "  -2.79365480e-02 -2.85745095e-02 -2.90765222e-02 -2.27117538e-02]\n",
      " [ 5.45540974e-02  3.58216166e-02  4.29796465e-02 -4.28774133e-02\n",
      "  -2.53856517e-02 -4.96740006e-02 -6.85328618e-02  1.26446197e-02\n",
      "  -6.29068315e-02  9.16511228e-04 -1.13217600e-01  6.12816475e-02\n",
      "   4.07069139e-02  2.07023583e-02  6.29070997e-02  2.39533954e-03\n",
      "  -4.55019325e-02  3.34208831e-02 -5.36634307e-03  1.90532161e-03]\n",
      " [ 5.84443286e-02  8.03893581e-02  7.74453878e-02 -4.52433899e-03\n",
      "  -3.99760976e-02  3.36480364e-02  2.42204536e-02  5.36067365e-03\n",
      "  -8.84324312e-02 -4.33721906e-03  2.23579467e-03  7.53161171e-03\n",
      "   5.50937839e-02 -1.04046054e-02 -1.15653109e-02  1.33515836e-03\n",
      "  -4.86288173e-03 -5.51207399e-04 -1.36218183e-02  4.24975939e-02]\n",
      " [ 7.53992870e-02  9.53488518e-03  8.35392028e-02  1.24013722e-02\n",
      "  -4.35941182e-02  3.20862457e-02 -9.93988104e-03  3.64958905e-02\n",
      "   5.92818158e-03 -8.51595327e-02 -4.52922657e-02  6.40492961e-02\n",
      "   3.56842875e-02 -1.78306997e-02  2.86328625e-02  1.70214567e-02\n",
      "  -1.51817948e-02 -4.26065102e-02 -5.86179420e-02 -3.66058089e-02]\n",
      " [ 2.26727948e-02  9.55216307e-03  3.04563642e-02 -3.08064241e-02\n",
      "   4.55774292e-02 -5.42949252e-02  1.05322981e-02  1.26903886e-02\n",
      "   5.87821901e-02  4.04968821e-02 -8.42084177e-03  1.10775597e-01\n",
      "   3.67632769e-02 -2.86130863e-03 -4.84939069e-02 -8.85166079e-02\n",
      "  -4.64313850e-02 -3.76174562e-02 -4.54571545e-02 -7.87542295e-03]\n",
      " [ 3.19464616e-02  1.78711880e-02  2.90358998e-03  2.05215961e-02\n",
      "  -9.06752516e-03 -6.60339594e-02 -2.51951478e-02 -1.78017560e-02\n",
      "  -2.76383068e-02 -5.53394891e-02 -2.50638481e-02  6.83954656e-02\n",
      "   1.40174374e-01 -1.60605169e-03 -9.95970517e-03 -7.60652274e-02\n",
      "  -3.97318602e-02 -7.58929877e-03  3.44501846e-02  1.72474179e-02]\n",
      " [ 8.59212130e-02  7.87173063e-02  7.47003779e-02  2.55316552e-02\n",
      "  -1.04971729e-01  9.46351960e-02  4.65207873e-03 -2.92493068e-02\n",
      "  -3.26794200e-02 -2.25817747e-02  5.63875362e-02 -4.51843105e-02\n",
      "   3.20423692e-02  3.24886926e-02 -1.14123918e-01  5.29465601e-02\n",
      "   1.28209423e-02 -6.80314451e-02 -1.57493353e-02 -2.19884552e-02]\n",
      " [ 1.67580564e-02  5.64386547e-02  1.17447853e-01  1.54406540e-02\n",
      "  -1.95225992e-03  6.23813132e-03 -3.44879068e-02  1.11711985e-02\n",
      "   1.66207887e-02  3.78489681e-03 -3.51358280e-02  1.87756773e-02\n",
      "   9.08298511e-03  5.49941920e-02 -5.99467605e-02  8.11901242e-02\n",
      "  -6.80772811e-02 -7.07221255e-02  2.97008436e-02  4.49206308e-03]\n",
      " [ 3.04156300e-02  8.12231079e-02  6.27569258e-02  3.71007100e-02\n",
      "  -6.00118637e-02 -2.81896032e-02  1.85621884e-02  1.08561413e-02\n",
      "  -9.73974690e-02 -6.12191968e-02  2.18773801e-02  2.17659678e-02\n",
      "   9.18132216e-02 -1.32505810e-02 -6.60785567e-03 -1.13696260e-02\n",
      "  -3.25523084e-03  1.58376731e-02 -2.62788702e-02  4.22067270e-02]\n",
      " [ 3.79576813e-03  6.11740947e-02  5.35530001e-02 -1.86051838e-02\n",
      "  -5.52977361e-02 -1.43789304e-02 -1.93469524e-02 -1.96438860e-02\n",
      "   1.05392328e-02  7.47409323e-03  3.73032014e-03  5.03689088e-02\n",
      "   4.19373773e-02  1.69054419e-02 -4.02379297e-02 -3.20425257e-02\n",
      "  -4.51899925e-03 -3.78435701e-02 -6.53194711e-02 -5.00670448e-03]\n",
      " [ 9.59143043e-02  3.07977553e-02  4.61123027e-02  3.92391197e-02\n",
      "  -4.14406471e-02 -8.56211409e-03 -3.36306579e-02  7.23219709e-04\n",
      "  -5.86351231e-02  1.02002760e-02 -2.11024322e-02  4.64209765e-02\n",
      "  -4.39173058e-02  4.32983749e-02  5.80849014e-02 -3.29202935e-02\n",
      "   6.42627254e-02 -6.42468501e-03 -6.53926432e-02  4.89339083e-02]\n",
      " [ 2.98823696e-02  7.13001043e-02 -4.32746997e-03  2.86034588e-02\n",
      "  -7.41737485e-02  2.06623729e-02  1.01552755e-02  7.50310998e-03\n",
      "  -6.13902546e-02  1.48556093e-02 -3.70389633e-02 -6.91707199e-03\n",
      "  -3.84687670e-02  2.66802330e-02 -2.30227709e-02  1.06895529e-01\n",
      "  -6.84059188e-02 -7.09601715e-02  1.15516894e-02  1.83801185e-02]\n",
      " [ 5.16598709e-02  4.70653921e-02  1.24473125e-01  2.97112521e-02\n",
      "  -4.46570888e-02 -2.38406545e-04  5.81220873e-02 -3.06813587e-02\n",
      "  -3.03341076e-02 -2.30392013e-02 -5.61779216e-02  3.07005513e-02\n",
      "   6.05550446e-02  3.56944762e-02  6.55797776e-03 -4.10320796e-02\n",
      "   9.94625594e-03 -8.75295773e-02 -6.11777343e-02 -1.90639298e-03]\n",
      " [ 7.63386115e-02  8.42998177e-02  1.03588007e-01 -1.19972769e-02\n",
      "   1.74421575e-02 -2.09690593e-02 -3.75097804e-02 -8.91408846e-02\n",
      "  -1.49340453e-02 -1.75215211e-02  7.49226799e-03  4.95130243e-03\n",
      "   3.73360775e-02  9.38096121e-02 -2.62117330e-02  3.34632117e-03\n",
      "   7.16162845e-03 -5.32631576e-02 -6.34962022e-02 -1.92151386e-02]\n",
      " [-3.62432785e-02  8.66631716e-02  3.15666124e-02 -8.15705117e-03\n",
      "  -2.45936736e-02 -6.77332878e-02 -3.10583189e-02 -2.99368165e-02\n",
      "  -8.31030980e-02 -1.35666570e-02 -3.24608423e-02  2.37372015e-02\n",
      "   3.48864719e-02  5.22862971e-02  3.54536884e-02 -1.90093853e-02\n",
      "   4.64618206e-02 -7.30908290e-02  3.16203968e-03  6.78965822e-02]\n",
      " [ 2.16722768e-02  5.23211397e-02  1.06488660e-01 -3.83066647e-02\n",
      "   1.57564096e-02  1.08515821e-01 -5.91912493e-03 -2.16027386e-02\n",
      "  -1.06496550e-02  9.19453707e-03  1.78404655e-02  2.48789936e-02\n",
      "  -3.35189258e-03  1.83853544e-02 -3.03035863e-02 -6.05470538e-02\n",
      "  -4.10985202e-02 -7.89052173e-02  1.16621358e-02 -4.44142856e-02]]\n",
      "(50, 512)\n"
     ]
    }
   ],
   "source": [
    "# Directly get audio embeddings from audio files\n",
    "\n",
    "audio_embed = model.get_audio_embedding_from_filelist(x = audio_file[:50], use_tensor=False)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24082b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 512)\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# with open('data/clap_audio_embeds.pkl', 'wb') as f:\n",
    "#     pickle.dump((audio_embed), f)\n",
    "# with open('data/clap_audio_embeds.pkl', 'rb') as f:\n",
    "#     audio_embed = pickle.load(f)\n",
    "# print(audio_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954f2fc",
   "metadata": {},
   "source": [
    ">## 3) Extract Text Embedding\n",
    ": 총 2000개 중 100개만 넣었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2512b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "['dog', 'chirping_birds', 'vacuum_cleaner', 'vacuum_cleaner', 'thunderstorm', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'door_wood_knock']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 DataFrame으로 로드\n",
    "df = pd.read_csv(\"ESC-50-master/ESC-50-master/meta/esc50.csv\")\n",
    "\n",
    "# 'category' column의 모든 데이터를 list로 받음\n",
    "category_data = df['category'].tolist()\n",
    "\n",
    "# 출력\n",
    "print(len(category_data))\n",
    "print(category_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf1489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00751023 -0.00121427  0.01298879 ...  0.02761818  0.00774936\n",
      "   0.01292299]\n",
      " [ 0.04226422  0.08132149 -0.00011061 ... -0.02582695 -0.02809787\n",
      "  -0.05834977]\n",
      " [ 0.00394155  0.0236528  -0.01777841 ...  0.05986715  0.03546726\n",
      "   0.00757419]\n",
      " ...\n",
      " [ 0.06618918  0.07592878 -0.03068296 ...  0.01541831 -0.02546537\n",
      "  -0.08116932]\n",
      " [ 0.06618918  0.07592878 -0.03068296 ...  0.01541831 -0.02546537\n",
      "  -0.08116932]\n",
      " [-0.06413938  0.03432493 -0.01238014 ...  0.01617089 -0.00911491\n",
      "  -0.07885502]]\n",
      "(50, 512)\n"
     ]
    }
   ],
   "source": [
    "# Get text embedings from texts:\n",
    "text_data = category_data\n",
    "text_embed = model.get_text_embedding(text_data[:50])\n",
    "print(text_embed)\n",
    "print(text_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49de8a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 512)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('data/clap_text_embeds.pkl', 'wb') as f:\n",
    "    pickle.dump((text_embed), f)\n",
    "with open('data/clap_text_embeds.pkl', 'rb') as f:\n",
    "    text_embed = pickle.load(f)\n",
    "print(text_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "norm_text = np.linalg.norm(text_embed, axis=1, keepdims=True)\n",
    "norm_audio = np.linalg.norm(audio_embed, axis=1, keepdims=True)\n",
    "dot_product = np.dot(text_embed, audio_embed.T)\n",
    "cos_similarity = dot_product / (norm_text * norm_audio.T)\n",
    "\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESC50_Model_Cosine_Similarlity [:100]\n",
    "\n",
    "# Compute row-wise mean\n",
    "row_means = np.mean(cos_similarity, axis=1)\n",
    "# Compute overall mean\n",
    "overall_mean = np.mean(row_means)\n",
    "\n",
    "print(\"Overall mean cosine similarity:\", overall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d1739",
   "metadata": {},
   "source": [
    "# 2. Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a587083",
   "metadata": {},
   "source": [
    ">## 1) Load Model (Dataset : ESC50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b690d9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.weight \t Loaded\n",
      "audio_branch.patch_embed.mel_conv2d.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.0.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.3.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.local_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.1.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.2.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.4.bias \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.weight \t Loaded\n",
      "audio_branch.patch_embed.fusion_model.global_att.5.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "import laion_clap\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "# download https://drive.google.com/drive/folders/1scyH43eQAcrBz-5fAw44C6RNBhC3ejvX?usp=sharing and extract ./ESC50_1/test/0.tar to ./ESC50_1/test/\n",
    "\n",
    "# Pick a Dataset type\n",
    "esc50_test_dir = 'data/0.tar/*/' # (flac-json)\n",
    "class_index_dict_path = 'label/ESC50_class_labels_indices_space.json'\n",
    "\n",
    "# Load the model\n",
    "model = laion_clap.CLAP_Module(enable_fusion=True) # device=device\n",
    "model.load_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d7ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 갯수: 50\n",
      "dog 0\n",
      "rooster 1\n",
      "pig 2\n",
      "cow 3\n",
      "frog 4\n",
      "cat 5\n",
      "hen 6\n",
      "insects 7\n",
      "sheep 8\n",
      "crow 9\n"
     ]
    }
   ],
   "source": [
    "# Get the class index dict\n",
    "class_index_dict = {v: k for v, k in json.load(open(class_index_dict_path)).items()}\n",
    "print('class 갯수:',len(class_index_dict))\n",
    "for key, value in list(class_index_dict.items())[:10]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63111928",
   "metadata": {},
   "source": [
    ">## 2) Load Audio / Text / Ground-truth files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ab0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data\n",
    "import tarfile\n",
    "\n",
    "# 압축 파일 해제\n",
    "# with tarfile.open('data/0.tar') as tar:\n",
    "#     tar.extractall('data/ESC50_1/test/')\n",
    "\n",
    "# # flac 파일 경로 가져오기\n",
    "audio_files = sorted(glob.glob('data/ESC50_1/test/**/*.flac', recursive=True))\n",
    "\n",
    "print('audio files 갯수:',len(audio_files)) # 여러개 파일 \n",
    "print(audio_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11738266",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = sorted(glob.glob('./data/ESC50_1/test/**/*.json', recursive=True))\n",
    "print('json_files 갯수:',len(json_files))\n",
    "print(json_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf95c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_idx = [class_index_dict[json.load(open(jf))['category']] for jf in json_files]\n",
    "print('ground_truth_idx 갯수:',len(ground_truth_idx))\n",
    "print(ground_truth_idx[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8628c",
   "metadata": {},
   "source": [
    ">## 3) Zeroshot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "torchaudio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4faf82e",
   "metadata": {},
   "source": [
    ">>### (1) Audio/Text Embedding 값 불러오기 & 간단스크립트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ground_truth = torch.tensor(ground_truth_idx).view(-1, 1)\n",
    "\n",
    "    # Get text features\n",
    "    all_texts = [\"This is a sound of \" + t for t in class_index_dict.keys()]\n",
    "    text_embed = model.get_text_embedding(all_texts)\n",
    "    audio_embed = model.get_audio_embedding_from_filelist(x=audio_files)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61f5ae",
   "metadata": {},
   "source": [
    "- ground_truth_idx 리스트(classification 답)를 PyTorch 텐서로 변환하고,.view로 차원을 맞춰둠.\n",
    "- all texts 인자에 class_index_dict의 key값(text)를 받아 확인할 수 있게 하고,\n",
    "- text인 all texts 리스트에 대한 임베딩 값 text_embed 얻고,\n",
    "- audio files 리스트에 있는 모든 오디오 파일에 대한 audio_embed 얻음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('text_embed length :',len(text_embed)) # class 갯수\n",
    "    print('text_embed shape :',text_embed.shape)\n",
    "    print(text_embed[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('audio_embed length :',len(audio_embed)) # 오디오파일 갯수\n",
    "    print('audio_embed shape :',audio_embed.shape)\n",
    "    print(audio_embed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a3733",
   "metadata": {},
   "source": [
    ">>### (2) Contrastive Learning & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd57e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ranking = torch.argsort(torch.tensor(audio_embed) \n",
    "                            @ torch.tensor(text_embed).t(), \n",
    "                            descending=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26225cf",
   "metadata": {},
   "source": [
    "1. audio_embed & text_embed.T 내적곱해서 latent space 통일\n",
    "2. torch.argsort : 계산한 내적값 작은순으로 sort해서 다시 index 부여\n",
    "3. 다시 내림차순으로 정렬  \n",
    "**>>> ranking = 음성과 텍스트 데이터간 유사도가 높은 순서대로 정렬된 인덱스가 담김**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "    preds = torch.where(ranking == ground_truth)[1]\n",
    "    preds = preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4baa5",
   "metadata": {},
   "source": [
    "1. 가장 유사하다고 판단한 ranking 값과 실제답 ground_truth가 같으면 true를 반환함\n",
    "2. true값의 위치를 찾아 주는 것이 torch.where\n",
    "3.  [1] = 인덱스값   \n",
    "**>>> preds = 음성과 텍스트간의 유사도가 가장 높은 class name(text)를 담음**\n",
    "4. tensor를 np로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1a9bf",
   "metadata": {},
   "source": [
    ">>### (3) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    metrics = {}\n",
    "    metrics[f\"mean_rank\"] = preds.mean() + 1\n",
    "    metrics[f\"median_rank\"] = np.floor(np.median(preds)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(metrics[f\"mean_rank\"])\n",
    "    print(metrics[f\"median_rank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbbb6b",
   "metadata": {},
   "source": [
    "1. 결과값을 담을 metrics dictionary 열어두고\n",
    "2. preds의 평균값과 중앙값을 구해서 순위를 0이 아닌 1부터 시작하게함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ce393",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for k in [1, 5, 10]:\n",
    "        metrics[f\"R@{k}\"] = np.mean(preds < k)\n",
    "    # map@10\n",
    "    metrics[f\"mAP@10\"] = np.mean(np.where(preds < 10, 1 / (preds + 1), 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a1931",
   "metadata": {},
   "source": [
    "- R@1,5,10 = 예측된 순위(preds)가 실상위 k개 안에 있으면 1, 아니면 0 한 뒤 평균\n",
    "- mAP@10 = Mean Average Precision at 10 : 모델이 상위 10개 예측에 대해 얼마나 정확한 결과를 내는지 평가하는 지표\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ce17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\n",
    "        f\"Zeroshot Classification Results: \"\n",
    "        + \"\\t\".join([f\"{k}: {round(v, 4):.4f}\" for k, v in metrics.items()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e542d4",
   "metadata": {},
   "source": [
    "- mean_rank: 1.2425 : 모델이 예측한 결과 중 상위 2개 클래스가 정답 클래스로 예측되었음.\n",
    "- median_rank: 1.0 : 중앙값으로 예측해도 평균적으로 1이라는 것은 대체로 예측된 클래스 중 1위 클래스가 실제값이라는 뜻.\n",
    "- R@1,5,10 : 예측한 1,5,10위까지의 순위 안에 정답이 포함될 확률\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e65df5",
   "metadata": {},
   "source": [
    ">>### (4) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ranking))\n",
    "print(type(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    # num_classes x num_classes 크기의 0으로 채워진 행렬 생성\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # 행렬의 각 [i][j] 요소는 i번 클래스가 j번 클래스로 분류된 샘플의 수\n",
    "    for i in range(len(y_true)):\n",
    "        true_idx = y_true[i]\n",
    "        pred_idx = y_pred[i]\n",
    "        conf_mat[true_idx][pred_idx] += 1\n",
    "\n",
    "    return conf_mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, class_names):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp = disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation=90)  # x축 레이블을 90도 기울임\n",
    "    plt.show()\n",
    "\n",
    "# confusion matrix 생성\n",
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, torch.argmax(ranking, dim=1), num_classes)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "plot_confusion_matrix(conf_mat, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fa2cb",
   "metadata": {},
   "source": [
    "hand saw - dog(8) / cat - hand saw(6) / chainsaw - rooster(6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a163b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "wav_path = './data/dog.wav'\n",
    "audio = AudioSegment.from_file(wav_path, format=\"wav\")\n",
    "play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66484b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    # num_classes x num_classes 크기의 0으로 채워진 행렬 생성\n",
    "    conf_mat = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    # 행렬의 각 [i][j] 요소는 i번 클래스가 j번 클래스로 분류된 샘플의 수\n",
    "    for i in range(len(y_true)):\n",
    "        true_idx = y_true[i]\n",
    "        pred_idx = y_pred[i]\n",
    "        conf_mat[true_idx][pred_idx] += 1\n",
    "\n",
    "    return conf_mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, class_names):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(conf_mat, interpolation='nearest', cmap='Blues')\n",
    "    \n",
    "    # x, y 축 설정\n",
    "    ax.set(xticks=np.arange(conf_mat.shape[1]), yticks=np.arange(conf_mat.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    # x 축 레이블 90도 회전\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # 각 셀에 값 출력\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            text = ax.text(j, i, conf_mat[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    # colorbar 추가\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # 그래프 보여주기\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# confusion matrix 생성\n",
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, torch.argmax(ranking, dim=1), num_classes)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "plot_confusion_matrix(conf_mat, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8efd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23268637",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_index_dict)\n",
    "conf_mat = confusion_matrix(ground_truth, ranking, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f277de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# 오차행렬 생성\n",
    "conf_mat = multilabel_confusion_matrix(ground_truth, ranking)\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62eef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 예측값과 실제값이 일치하는지 계산\n",
    "correct_preds = (ranking == ground_truth)\n",
    "print(correct_preds.shape)\n",
    "incorrect_preds = (ranking != ground_truth)\n",
    "incorrect_ranking = ranking[incorrect_preds][0]\n",
    "print(incorrect_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ba29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 오차행렬 생성\n",
    "conf_mat = confusion_matrix(ground_truth[incorrect_preds], ranking[incorrect_preds])\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "class_names = list(class_index_dict.keys())\n",
    "\n",
    "# 오차행렬 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False, ax=ax)\n",
    "\n",
    "# x축, y축 라벨 설정\n",
    "ax.set_xlabel('Predicted labels', fontsize=16)\n",
    "ax.set_ylabel('True labels', fontsize=16)\n",
    "ax.set_xticklabels(class_names, fontsize=12)\n",
    "ax.set_yticklabels(class_names, fontsize=12)\n",
    "\n",
    "# 그래프 제목 설정\n",
    "ax.set_title('Confusion Matrix', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17deb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a34b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일치하는 경우 0, 그렇지 않은 경우 1 array 만들기\n",
    "color_indices = np.where(correct_preds, 0, 1)\n",
    "color_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3603c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 100))\n",
    "im = ax.imshow(color_indices, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xticks(np.arange(color_indices.shape[1]))\n",
    "ax.set_yticks(np.arange(color_indices.shape[0]))\n",
    "ax.set_xticklabels(class_index_dict, fontsize=8)\n",
    "ax.set_yticklabels(range(400), fontsize=8)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf59e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946db9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [class_index_dict[i] for i in preds]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49827538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class_index_dict_path = 'data/ESC50_class_labels_indices_space.json'\n",
    "class_index_dict = {v: k for v, k in json.load(open(class_index_dict_path)).items()}\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'preds': preds,\n",
    "    'ground_truth': ground_truth.squeeze().tolist(),\n",
    "    'audio_file': audio_files,\n",
    "    'actual_class': [class_index_dict[i] for i in ground_truth.squeeze().tolist()],\n",
    "    'predicted_class': [class_index_dict[i] for i in preds]\n",
    "})\n",
    "\n",
    "df_diff = df[df['actual_class'] != df['predicted_class']]\n",
    "display(df_diff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af62dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
